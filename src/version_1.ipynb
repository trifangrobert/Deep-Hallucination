{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss, Linear, ReLU, Sequential\nimport cv2\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/unibuc-ml-202325/\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nhyperparameters = {\n    'batch_size': 64,\n    'learning_rate': 0.001,\n    'epochs': 15,\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.343163Z","iopub.execute_input":"2023-05-28T12:23:19.343859Z","iopub.status.idle":"2023-05-28T12:23:19.351616Z","shell.execute_reply.started":"2023-05-28T12:23:19.343824Z","shell.execute_reply":"2023-05-28T12:23:19.350409Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, img_dir, csv_file, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.img_labels = pd.read_csv(csv_file)\n    def __len__(self):\n        return len(self.img_labels)\n    def __getitem__(self, index):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[index, 0])\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.img_labels.iloc[index, 1]\n        if self.transform:\n            image = self.transform(image)\n        return (image, label)    ","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.353268Z","iopub.execute_input":"2023-05-28T12:23:19.353769Z","iopub.status.idle":"2023-05-28T12:23:19.362609Z","shell.execute_reply.started":"2023-05-28T12:23:19.353728Z","shell.execute_reply":"2023-05-28T12:23:19.361423Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"class TestImageDataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        img_id = self.df.iloc[index, 0]\n        img_path = os.path.join(self.img_dir, img_id)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            image = self.transform(image)\n        return (image, img_id)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.365450Z","iopub.execute_input":"2023-05-28T12:23:19.365832Z","iopub.status.idle":"2023-05-28T12:23:19.377034Z","shell.execute_reply.started":"2023-05-28T12:23:19.365798Z","shell.execute_reply":"2023-05-28T12:23:19.376084Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToPILImage(), transforms.Resize((64, 64)), transforms.ToTensor()])\n\ntrain_dataset = CustomImageDataset(img_dir=DATA_PATH + \"train_images\", csv_file=DATA_PATH + \"train.csv\", transform=transform)\nval_dataset = CustomImageDataset(img_dir=DATA_PATH + \"val_images\", csv_file=DATA_PATH + \"val.csv\", transform=transform)\ntest_dataset = TestImageDataset(img_dir=DATA_PATH + \"test_images\", csv_file=DATA_PATH + \"test.csv\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.378533Z","iopub.execute_input":"2023-05-28T12:23:19.379273Z","iopub.status.idle":"2023-05-28T12:23:19.406868Z","shell.execute_reply.started":"2023-05-28T12:23:19.379222Z","shell.execute_reply":"2023-05-28T12:23:19.406047Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# for i in test_loader:\n#     print(i)\n#     break","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.408018Z","iopub.execute_input":"2023-05-28T12:23:19.408702Z","iopub.status.idle":"2023-05-28T12:23:19.412638Z","shell.execute_reply.started":"2023-05-28T12:23:19.408669Z","shell.execute_reply":"2023-05-28T12:23:19.411562Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, num_classes=96):\n        super(Net, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        self.classsifier = nn.Sequential(\n            nn.Linear(512*2*2, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1) # flatten all dimensions except batch, you can also use x.view(x.size(0), -1)\n        x = self.classsifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.413914Z","iopub.execute_input":"2023-05-28T12:23:19.414694Z","iopub.status.idle":"2023-05-28T12:23:19.428147Z","shell.execute_reply.started":"2023-05-28T12:23:19.414661Z","shell.execute_reply":"2023-05-28T12:23:19.427215Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"def train_model(train_loader, val_loader, hyperparameters):\n    model = Net().to(device)\n    criterion = CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n    num_epochs = hyperparameters['epochs']\n\n    for epoch in range(num_epochs):\n        print(\"Starting epoch: \", epoch)\n        # set the model to train mode\n        # enable dropout, batch normalization etc.\n        model.train()\n#         print(len(train_loader))\n#         step = 0\n        for images, labels in train_loader:\n#             step += 1\n#             if (step % 50 == 0):\n#                 print(\"Step: \", step)\n            # loads the images to cuda if available\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images) # forward pass\n            loss = criterion(outputs, labels) # compute the loss\n\n            optimizer.zero_grad() # reset the gradients because they accumulate by default\n            loss.backward() # compute the gradients in the backward pass\n            optimizer.step() # update the parameters based on the gradients computed in the backward pass\n        print(\"Validating data...\")\n        # set the model to evaluation mode\n        # disable dropout, batch normalization etc.\n        model.eval()\n        with torch.no_grad(): # to disable gradient calculation and backpropagation\n            correct = 0\n            total = 0\n            for images, labels in val_loader:\n                # loads the images to cuda if availabl\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images) # forward pass\n                # torch.max returns a tuple (values, indices) where indices is the index of the maximum value of a tensor along a dimension\n                _, predicted = torch.max(outputs.data, 1) # get the predicted class with highest probability\n                total += labels.size(0) # total number of labels in a batch\n                correct += (predicted == labels).sum().item() # total correct predictions\n\n            print('Epoch [{}/{}], Validation Accuracy: {:.2f}%'\n                  .format(epoch+1, num_epochs, 100 * correct / total))\n    \n    return model, optimizer\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.435928Z","iopub.execute_input":"2023-05-28T12:23:19.436560Z","iopub.status.idle":"2023-05-28T12:23:19.447932Z","shell.execute_reply.started":"2023-05-28T12:23:19.436521Z","shell.execute_reply":"2023-05-28T12:23:19.446924Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    state = {'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n    torch.save(state, path)\n\ndef load_model(model, optimizer, path):\n    state = torch.load(path)\n    model.load_state_dict(state['model'])\n    optimizer.load_state_dict(state['optimizer'])\n    return model, optimizer","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.452641Z","iopub.execute_input":"2023-05-28T12:23:19.452958Z","iopub.status.idle":"2023-05-28T12:23:19.461378Z","shell.execute_reply.started":"2023-05-28T12:23:19.452933Z","shell.execute_reply":"2023-05-28T12:23:19.460452Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_loader):\n    model.eval()\n    predicted_labels = []\n    image_ids = []\n    with torch.no_grad(): # to disable gradient calculation and backpropagation\n        for images, ids in test_loader:\n            images = images.to(device)\n\n            outputs = model(images) # forward pass\n\n            _, predicted = torch.max(outputs.data, 1) # get the predicted class with highest probability\n\n            predicted_labels.extend(predicted.tolist())\n            image_ids.extend(ids)\n    return predicted_labels, image_ids","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.463145Z","iopub.execute_input":"2023-05-28T12:23:19.463840Z","iopub.status.idle":"2023-05-28T12:23:19.473764Z","shell.execute_reply.started":"2023-05-28T12:23:19.463787Z","shell.execute_reply":"2023-05-28T12:23:19.472761Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# print(torch.cuda.is_available())\n# print(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:23:19.475359Z","iopub.execute_input":"2023-05-28T12:23:19.475813Z","iopub.status.idle":"2023-05-28T12:23:19.483791Z","shell.execute_reply.started":"2023-05-28T12:23:19.475780Z","shell.execute_reply":"2023-05-28T12:23:19.482832Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"model, optimizer = train_model(train_loader, val_loader, hyperparameters)\n# save_model(model, optimizer,  + \"models/model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:24:34.735067Z","iopub.execute_input":"2023-05-28T12:24:34.735771Z","iopub.status.idle":"2023-05-28T12:26:02.910735Z","shell.execute_reply.started":"2023-05-28T12:24:34.735736Z","shell.execute_reply":"2023-05-28T12:26:02.909133Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"Starting epoch:  0\nValidating data...\nEpoch [1/15], Validation Accuracy: 3.60%\nStarting epoch:  1\nValidating data...\nEpoch [2/15], Validation Accuracy: 3.60%\nStarting epoch:  2\nValidating data...\nEpoch [3/15], Validation Accuracy: 3.60%\nStarting epoch:  3\nValidating data...\nEpoch [4/15], Validation Accuracy: 3.60%\nStarting epoch:  4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# save_model(model, optimizer,  + \"models/model.pth\")\u001b[39;00m\n","Cell \u001b[0;32mIn[106], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, hyperparameters)\u001b[0m\n\u001b[1;32m     11\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         print(len(train_loader))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         step = 0\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#             step += 1\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#             if (step % 50 == 0):\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#                 print(\"Step: \", step)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;66;03m# loads the images to cuda if available\u001b[39;00m\n\u001b[1;32m     19\u001b[0m             images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m             labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[101], line 14\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 14\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (image, label)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    165\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 166\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:524\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyaccess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exif \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    526\u001b[0m         deprecate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage categories\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_animated\u001b[39m\u001b[38;5;124m\"\u001b[39m, plural\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"predicted_labels, image_ids = test_model(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:24:32.650812Z","iopub.status.idle":"2023-05-28T12:24:32.651508Z","shell.execute_reply.started":"2023-05-28T12:24:32.651210Z","shell.execute_reply":"2023-05-28T12:24:32.651256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(predicted_labels[:10], image_ids[:10])","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:24:32.653575Z","iopub.status.idle":"2023-05-28T12:24:32.654042Z","shell.execute_reply.started":"2023-05-28T12:24:32.653802Z","shell.execute_reply":"2023-05-28T12:24:32.653825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_predictions = pd.DataFrame({\n    'Image': image_ids,\n    'Class': predicted_labels\n})\n\n# Save the DataFrame to a CSV file\ndf_predictions.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-28T12:24:32.655730Z","iopub.status.idle":"2023-05-28T12:24:32.656189Z","shell.execute_reply.started":"2023-05-28T12:24:32.655953Z","shell.execute_reply":"2023-05-28T12:24:32.655975Z"},"trusted":true},"execution_count":null,"outputs":[]}]}