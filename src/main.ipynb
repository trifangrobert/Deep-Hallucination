{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss, Linear, ReLU, Sequential\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "hyperparameters = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_labels = pd.read_csv(csv_file)\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[index, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.img_labels.iloc[index, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.df.iloc[index, 0]\n",
    "        img_path = os.path.join(self.img_dir, img_id)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset(img_dir=DATA_PATH + \"train_images\", csv_file=DATA_PATH + \"train.csv\", transform=original_transform)\n",
    "val_dataset = CustomImageDataset(img_dir=DATA_PATH + \"val_images\", csv_file=DATA_PATH + \"val.csv\", transform=original_transform)\n",
    "test_dataset = TestImageDataset(img_dir=DATA_PATH + \"test_images\", csv_file=DATA_PATH + \"test.csv\", transform=original_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.4471, 0.5020, 0.5961,  ..., 0.3686, 0.4471, 0.4471],\n",
      "         [0.4745, 0.5294, 0.5176,  ..., 0.3608, 0.4314, 0.4510],\n",
      "         [0.4745, 0.5333, 0.4588,  ..., 0.3686, 0.2863, 0.4235],\n",
      "         ...,\n",
      "         [0.4588, 0.5020, 0.5098,  ..., 0.2706, 0.4471, 0.4588],\n",
      "         [0.4588, 0.5961, 0.4667,  ..., 0.2941, 0.3765, 0.4392],\n",
      "         [0.4902, 0.5843, 0.5843,  ..., 0.4627, 0.4863, 0.4745]],\n",
      "\n",
      "        [[0.4588, 0.5333, 0.6157,  ..., 0.4039, 0.4667, 0.4314],\n",
      "         [0.4745, 0.5608, 0.5373,  ..., 0.3882, 0.4510, 0.4275],\n",
      "         [0.4745, 0.5686, 0.4902,  ..., 0.3843, 0.3137, 0.4118],\n",
      "         ...,\n",
      "         [0.4549, 0.5176, 0.5529,  ..., 0.2824, 0.4588, 0.4510],\n",
      "         [0.4627, 0.6314, 0.4980,  ..., 0.3137, 0.4000, 0.4314],\n",
      "         [0.4824, 0.6196, 0.6235,  ..., 0.4941, 0.5137, 0.4706]],\n",
      "\n",
      "        [[0.3882, 0.4510, 0.4863,  ..., 0.3373, 0.3961, 0.3451],\n",
      "         [0.4157, 0.4667, 0.4039,  ..., 0.3255, 0.3804, 0.3451],\n",
      "         [0.4157, 0.4745, 0.3882,  ..., 0.3373, 0.2784, 0.3294],\n",
      "         ...,\n",
      "         [0.3686, 0.4235, 0.4353,  ..., 0.2392, 0.3529, 0.3647],\n",
      "         [0.3804, 0.5176, 0.4078,  ..., 0.2549, 0.3137, 0.3451],\n",
      "         [0.3922, 0.5098, 0.5020,  ..., 0.3922, 0.4000, 0.3686]]]), 76)\n",
      "(tensor([[[0.8235, 0.8471, 0.8510,  ..., 0.5882, 0.1412, 0.0824],\n",
      "         [0.8275, 0.8510, 0.8549,  ..., 0.6941, 0.1882, 0.0784],\n",
      "         [0.8314, 0.8510, 0.8510,  ..., 0.7765, 0.3059, 0.1412],\n",
      "         ...,\n",
      "         [0.8549, 0.8510, 0.8392,  ..., 0.6039, 0.6157, 0.6039],\n",
      "         [0.8588, 0.8431, 0.7059,  ..., 0.5569, 0.5569, 0.5529],\n",
      "         [0.8549, 0.7333, 0.4863,  ..., 0.5451, 0.5176, 0.5020]],\n",
      "\n",
      "        [[0.7451, 0.6784, 0.6392,  ..., 0.5922, 0.1412, 0.0745],\n",
      "         [0.7451, 0.6627, 0.6314,  ..., 0.7176, 0.2039, 0.0667],\n",
      "         [0.7255, 0.6471, 0.6314,  ..., 0.8471, 0.3765, 0.1451],\n",
      "         ...,\n",
      "         [0.7961, 0.8078, 0.8157,  ..., 0.4196, 0.4118, 0.4078],\n",
      "         [0.7882, 0.7961, 0.8314,  ..., 0.3961, 0.3961, 0.3882],\n",
      "         [0.7843, 0.8235, 0.7020,  ..., 0.4078, 0.3804, 0.3569]],\n",
      "\n",
      "        [[0.9686, 0.9843, 0.9804,  ..., 0.7020, 0.2000, 0.1137],\n",
      "         [0.9686, 0.9843, 0.9804,  ..., 0.8431, 0.2745, 0.1020],\n",
      "         [0.9686, 0.9843, 0.9843,  ..., 0.9490, 0.4667, 0.2078],\n",
      "         ...,\n",
      "         [0.9686, 0.9686, 0.9647,  ..., 0.5765, 0.5647, 0.5569],\n",
      "         [0.9686, 0.9686, 0.9686,  ..., 0.5529, 0.5490, 0.5412],\n",
      "         [0.9608, 0.9686, 0.7961,  ..., 0.5765, 0.5412, 0.5098]]]), 24)\n"
     ]
    }
   ],
   "source": [
    "# print first image and label\n",
    "print(train_dataset[1])\n",
    "print(val_dataset[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA Version is  11.7\n",
      "Whether CUDA is supported by our system: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch CUDA Version is \", torch.version.cuda)\n",
    "print(\"Whether CUDA is supported by our system:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=96):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classsifier = nn.Sequential(\n",
    "            nn.Linear(512*2*2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch, you can also use x.view(x.size(0), -1)\n",
    "        x = self.classsifier(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Validation Accuracy: 3.60%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels) \u001b[39m# compute the loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# reset the gradients because they accumulate by default\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# compute the gradients in the backward pass\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# update the parameters based on the gradients computed in the backward pass\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# disable dropout, batch normalization etc.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aienv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/aienv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "num_epochs = hyperparameters['epochs']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # set the model to train mode\n",
    "    # enable dropout, batch normalization etc.\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        # loads the images to cuda if availabl\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images) # forward pass\n",
    "        loss = criterion(outputs, labels) # compute the loss\n",
    "\n",
    "        optimizer.zero_grad() # reset the gradients because they accumulate by default\n",
    "        loss.backward() # compute the gradients in the backward pass\n",
    "        optimizer.step() # update the parameters based on the gradients computed in the backward pass\n",
    "\n",
    "    # set the model to evaluation mode\n",
    "    # disable dropout, batch normalization etc.\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # to disable gradient calculation and backpropagation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            # loads the images to cuda if availabl\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images) # forward pass\n",
    "            # torch.max returns a tuple (values, indices) where indices is the index of the maximum value of a tensor along a dimension\n",
    "            _, predicted = torch.max(outputs.data, 1) # get the predicted class with highest probability\n",
    "            total += labels.size(0) # total number of labels in a batch\n",
    "            correct += (predicted == labels).sum().item() # total correct predictions\n",
    "\n",
    "        print('Epoch [{}/{}], Validation Accuracy: {:.2f}%'\n",
    "              .format(epoch+1, num_epochs, 100 * correct / total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test torch.max\n",
    "# a = torch.randn(4, 4)\n",
    "# print(a)\n",
    "\n",
    "# x, y = torch.max(a, 0)\n",
    "# x, y = torch.max(a, 1)\n",
    "# print(x, y)\n",
    "\n",
    "#test labels.size(0) and convert tensor to int with .item()\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(labels, images)  \n",
    "#     print(labels.size(0))\n",
    "#     # create a tensor of size labels.size(0) with values from 1 to 96 and compare it with labels\n",
    "#     predicted = torch.tensor([i for i in range(1, labels.size(0)+1)])\n",
    "#     print(predicted)\n",
    "#     print((predicted == labels).sum().item())\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted_labels = []\n",
    "with torch.no_grad(): # to disable gradient calculation and backpropagation\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        outputs = model(images) # forward pass\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1) # get the predicted class with highest probability\n",
    "\n",
    "        predicted_labels.extend(predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, path):\n",
    "    state = {'model': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, path)\n",
    "\n",
    "# def load_model(model, optimizer, path):\n",
    "#     state = torch.load(path)\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     optimizer.load_state_dict(state['optimizer'])\n",
    "#     return model, optimizer\n",
    "def load_model(path):\n",
    "    state = torch.load(path)\n",
    "    return state['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, optimizer, '../models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # model.eval()\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad(): # to disable gradient calculation and backpropagation\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images) # forward pass\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1) # get the predicted class with highest probability\n",
    "\n",
    "            predicted_labels.extend(predicted.tolist())\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39m../models/model.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m predicted_labels \u001b[39m=\u001b[39m test_model(model, test_loader)\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(): \u001b[39m# to disable gradient calculation and backpropagation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m----> 7\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      9\u001b[0m         outputs \u001b[39m=\u001b[39m model(images) \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m) \u001b[39m# get the predicted class with highest probability\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# model = load_model('../models/model.pth')\n",
    "predicted_labels = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tester.txt\", \"rb\") as fp:   # Unpickling\n",
    "    tester = pickle.load(fp)\n",
    "\n",
    "print(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
